from models.backbone.ResNet import resnet50
from utils.anchors import generate_default_anchor_maps, hard_nms
from utils.clustering import *
from utils.search_transfer import *
from settings.setup_functions import get_topk_tokens

counter1 = 0
counter2 = 0
counter3 = 0
counter4 = 0

class CurriculumConv(nn.Module):
	def __init__(self, backbone, basic_dim=512, dim=2048, class_num=200, topn=4, input_size=14, label_smooth=0.1, init=2, alpha=0.5, gamma=2, img_size=448, stage=3, drop=0.5, train_ver='sum'):
		super().__init__()
		self.basic_dim = basic_dim
		self.dim = dim
		self.class_num = class_num
		self.topn = topn
		self.part = False
		self.stage = stage
		self.img_size = img_size
		self.pad_side = 224
		self.ECA = eca_layer()
		if self.dim == 768:
			self.PR = PartsResort(self.topn, self.dim)
		elif self.basic_dim == 512:
			self.PR = PartsResort(self.topn, self.dim // 2)
		self.SAE = SAEBlock(self.dim)
		_, edge_anchors, _ = generate_default_anchor_maps()  # (1614, 4)
		self.edge_anchors = (edge_anchors + self.pad_side).astype(int)
		self.input_size = input_size
		self.classifier_list = nn.ModuleList()
		self.qp_list = nn.ModuleList()
		self.init = init
		self.alpha = alpha
		self.gamma = gamma
		self.train_ver = train_ver
		self.linear = nn.Linear(self.dim // 2 * 3, self.dim // 2)
		if self.dim == 768:
			self.attention6 = Attention(dim=self.dim)
			self.layer_norm = nn.LayerNorm(self.dim)
		elif self.basic_dim == 512:
			self.attention6 = Attention(dim=self.dim // 2)
			self.layer_norm = nn.LayerNorm(self.dim // 2)
		self.conv = ConvBlock(self.basic_dim, self.dim)
		self.gmp = nn.AdaptiveMaxPool2d(1)
		self.gap = nn.AdaptiveAvgPool2d(1)
		self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
		for i in range(self.stage):
			self.classifier_list.append(Classifier(self.dim // 2, basic_dim, class_num))
			self.qp_list.append(QPClassifier(self.dim // 2, basic_dim, class_num))
		self.classifier_list.append(Classifier(self.dim // 2 * 3, basic_dim, class_num))
		self.qp_list.append(QPClassifier(self.dim // 2 * 3, basic_dim, class_num))

		self.criterion1 = [LabelSmoothingCrossEntropy(smoothing=3 * label_smooth),
		                   LabelSmoothingCrossEntropy(smoothing=2 * label_smooth),
		                   LabelSmoothingCrossEntropy(smoothing=1 * label_smooth),
		                   LabelSmoothingCrossEntropy(smoothing=0 * label_smooth)]

		self.criterion2 = [nn.CrossEntropyLoss(),
		                   nn.CrossEntropyLoss(),
		                   nn.CrossEntropyLoss(),
		                   nn.CrossEntropyLoss()]

		self.a1 = nn.Parameter(torch.FloatTensor([1.0]), requires_grad=True).cuda()

		self.attention.apply(self.init_weights)
		self.backbone = backbone

	def qp_loss(self, main_out, qp_out, label, loss_a):
		global counter1, counter2, counter3, counter4, loss_list
		B = main_out.size(0)
		tmp_score = F.softmax(qp_out, 1)
		main_score = F.softmax(main_out, 1)
		p_max, _ = torch.max(main_score, dim=-1)
		q_max, _ = torch.max(tmp_score, dim=-1)
		for i in range(B):
			loss_list = []
			sel_mask = torch.FloatTensor(len(tmp_score), self.class_num).zero_().cuda()
			sel_mask.scatter_(1, label.unsqueeze(1), 1.0)
			sel_mask.cuda()
			if p_max[i] < self.beta and q_max[i] < self.beta:
				counter3 += 1
				sum_score = self.alpha * main_score + (1 - self.alpha) * tmp_score
				qp_score = (3 * tmp_score - main_score) * self.alpha
				div_score = qp_score / sum_score
				div_score = div_score.detach()

				sel_prob = (div_score * sel_mask).sum(1).view(-1, 1)  # (B,C)-> B -> (B,1)
				sel_prob = torch.clamp(sel_prob, 1e-8, 1 - 1e-8)

				mem_focal = torch.pow(1 - sel_prob[i], self.gamma) * loss_a[i]
				if mem_focal > 0:
					loss_list.append(mem_focal)
				else:
					mem_focal = mem_focal + 1e-3
					loss_list.append(mem_focal)

			if p_max[i] > self.beta > q_max[i]:
				counter2 += 1
			if p_max[i] > self.beta and q_max[i] > self.beta:
				counter1 += 1
			if p_max[i] < self.beta < q_max[i]:
				counter4 += 1

		sum_score = self.alpha * main_score + (1 - self.alpha) * tmp_score
		qp_score = (3 * tmp_score - main_score) * self.alpha
		div_score = qp_score / sum_score
		div_score = div_score.detach()

		sel_mask = torch.FloatTensor(len(tmp_score), self.class_num).zero_().cuda()
		sel_mask.scatter_(1, label.unsqueeze(1), 1.0)
		sel_mask.cuda()

		sel_prob = (div_score * sel_mask).sum(1).view(-1, 1)  # (B,C)-> B -> (B,1)
		sel_prob = torch.clamp(sel_prob, 1e-8, 1 - 1e-8)

		mem_focal = torch.pow(1 - sel_prob, self.gamma) * loss_a
		loss_c = mem_focal.mean()
		return loss_c

		global div_score
		if self.train_ver == 'sum':
			sum_score = self.alpha * main_score + (1 - self.alpha) * tmp_score
			qp_score = (3 * tmp_score - main_score) * self.alpha
			div_score = qp_score / sum_score
			div_score = div_score.detach()

		else:
			mul_score = main_score * tmp_score
			div_score = (mul_score - 1 + tmp_score) / mul_score
			div_score = div_score.detach()

		sel_mask = torch.FloatTensor(len(tmp_score), self.class_num).zero_().cuda()
		if len(label.shape)==2:
		    pass
		else:
		    sel_mask.scatter_(1, label.unsqueeze(1), 1.0)
		sel_mask.scatter_(1, label.unsqueeze(1), 1.0)
		sel_mask.cuda()

		sel_prob = (div_score * sel_mask).sum(1).view(-1, 1)  # (B,C)-> B -> (B,1)
		sel_prob = torch.clamp(sel_prob, 1e-8, 1 - 1e-8)

		mem_focal = torch.pow(1 - sel_prob, self.gamma) * loss_a
		loss_c = mem_focal.mean()
		return loss_c

	def weights_head_init(self, m):
		if isinstance(m, nn.Linear):
			nn.init.normal_(m.bias, 0, 0.01)
			nn.init.normal_(m.weight, 0, 0.01)
		elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):
			nn.init.constant_(m.bias, 0)
			nn.init.constant_(m.weight, 1.0)

	def init_weights(self, m):
		if isinstance(m, (nn.Linear, nn.Conv2d)):
			nn.init.kaiming_normal_(m.weight)
			if isinstance(m, nn.Linear) and m.bias is not None:
				nn.init.constant_(m.bias, 0)
		elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):
			nn.init.constant_(m.bias, 0)
			nn.init.constant_(m.weight, 1.0)
		try:
			nn.init.constant_(self.head.weight, 0)
			nn.init.constant_(self.head.bias, 0)
		except:
			pass

	def forward(self, x, label=None, epoch=0, step=0, step_per_epoch=188, no_qp=False, no_cls=False, is_train=True):
		if no_qp:
			pass
		else:
			if label is not None:
				if (epoch % self.init) & (step % step_per_epoch) == 0:
					self.qp_list.apply(self.weights_head_init)
		B = x.shape[0]
		x_original = x
		x = self.backbone(x)
		if self.dim == 768:
			selected_tokens = get_topk_tokens(x[1], x[2])
			x = x[0]
			B = x[0].shape[0]
		elif self.basic_dim == 512 or self.basic_dim == 256:
			x = x[1:]
			f3 = x[-1]
		if self.part:
			if self.basic_dim == 512:
				rpn_score = self.SAE(f3.detach())
				all_cdds = [
					np.concatenate((x.reshape(-1, 1), self.edge_anchors.copy(), np.arange(0, len(x)).reshape(-1, 1)), axis=1) for x in rpn_score.data.cpu().numpy()]
				top_n_cdds = np.array(
					[hard_nms(x, self.topn, iou_thresh=0.25) for x in all_cdds])
				top_n_index = top_n_cdds[:, :, -1].astype(int)
				top_n_index = torch.from_numpy(top_n_index).long().to(x_original.device)
				top_n_prob = torch.gather(rpn_score, dim=1, index=top_n_index)

				part_imgs = torch.zeros([B, self.topn, 3, 224, 224]).to(x_original.device)
				x_pad = F.pad(x_original, (self.pad_side, self.pad_side, self.pad_side, self.pad_side), mode='constant', value=0)
				for i in range(B):
					for j in range(self.topn):
						[y0, x0, y1, x1] = top_n_cdds[i, j, 1:5].astype(int)
						part_imgs[i:i + 1, j] = F.interpolate(x_pad[i:i + 1, :, y0:y1, x0:x1],
						                                      size=(224, 224), mode='bilinear',
						                                      align_corners=True)

			if no_cls:
				part_imgs = part_imgs.reshape(B * self.topn, 3, 192, 192)
				part_group = self.backbone(part_imgs.detach())
				part_group = part_group[1:]
				part_group = self.conv(part_group)
				part = part_group[-1]
				part = self.gmp(part).reshape(B * self.topn, -1)

				x = self.conv(x)
				x = x[-1]
				x = self.gmp(x).reshape(B, -1)

				out_list1 = self.classifier_list[0](x)

				if label is not None:
					if not no_qp:
						out_list2 = self.qp_list[0](x.detach())
						out_list4 = self.qp_list[0](part.detach())
						out_list3 = self.classifier_list[0](part)
						label_parts = label.unsqueeze(1).repeat(1, self.topn).reshape(-1)

						loss_img1 = self.criterion2[0](out_list1, label)
						loss_img2 = self.criterion2[0](out_list2, label)
						loss_img3 = self.qp_loss(out_list1, out_list2, label, loss_img1)
						loss_img = loss_img1 + loss_img2 + loss_img3

						loss_part1 = self.criterion2[0](out_list3, label_parts)
						loss_part2 = self.criterion2[0](out_list4, label_parts)
						loss_part3 = self.qp_loss(out_list3, out_list4, label_parts, loss_part1)
						loss_parts = loss_part1 + loss_part2 + loss_part3

						loss_rank = ranking_loss(top_n_prob, list_loss(out_list3, label_parts).reshape(B, self.topn))

						loss = loss_img + loss_parts + loss_rank
						return out_list1, [loss, loss_img, loss_parts]

					else:
						out_list3 = self.classifier_list[0](part)
						label_parts = label.unsqueeze(1).repeat(1, self.topn).reshape(-1)

						loss_img = self.criterion2[0](out_list1, label)
						loss_parts = self.criterion2[0](out_list3, label_parts)

						loss_rank = ranking_loss(top_n_prob, list_loss(out_list3, label_parts).reshape(B, self.topn))

						loss = loss_img + loss_parts + loss_rank
						return out_list1, [loss, loss_img, loss_parts]
				else:
					return out_list1
			else:
				x[0] = self.ECA(x[0])
				x[1] = self.ECA(x[1])
				x[2] = self.ECA(x[2])
				if self.basic_dim == 512:
					x = self.conv(x)

					global_x = x[-1]
					part_imgs = part_imgs.reshape(B * self.topn, 3, 224, 224)
					part_group = self.backbone(part_imgs)
					part_group = part_group[1:]
					part_group = self.conv(part_group)
					f1, f2, f3 = part_group[0], part_group[1], part_group[2]
					f1 = self.gap(f1).reshape(B * self.topn, self.dim // 2, -1)
					f2 = self.gap(f2).reshape(B * self.topn, self.dim // 2, -1)
					f3 = self.gap(f3).reshape(B * self.topn, self.dim // 2, -1)
					t = torch.cat((f1, f2, f3), dim=-1)
					t = rearrange(t, '(b p) c t -> b (p t) c', b=B)
					p = t
					t = self.layer_norm(t)
					t = (1 - self.a1) * self.attention6(t, global_x.detach()) + self.a1 * p
					t = rearrange(t, 'b (p t) c -> b p t c', p=self.topn)
					f1, f2, f3 = torch.split(t, [1, 1, 1], dim=-2)
					f = f1.mean(-2), f2.mean(-2), f3.mean(-2)
					part_group = (self.gmp(p).reshape(B, self.topn, -1) for p in part_group)
					f1, f2, f3 = (a + b for a, b in zip(f, part_group))
					f1_part = rearrange(f1, 'b p c -> (b p) c')
					f2_part = rearrange(f2, 'b p c -> (b p) c')
					f3_part = rearrange(f3, 'b p c -> (b p) c')

					x[0] = self.gmp(x[0]).reshape(B, -1)
					x[1] = self.gmp(x[1]).reshape(B, -1)
					x[2] = self.gmp(x[2]).reshape(B, -1)

				if self.dim == 768:
					global_x = x[-1]
					n1, n2, n3 = torch.split(selected_tokens, [1, 1, 1], dim=-2)
					f1 = n1.reshape(B, self.dim, -1)
					f2 = n2.reshape(B, self.dim, -1)
					f3 = n3.reshape(B, self.dim, -1)
					t = torch.cat((f1, f2, f3), dim=-1)
					t = rearrange(t, 'b c t -> b t c', b=B)
					p = t
					t = self.layer_norm(t)
					t = (1 - self.a1) * self.attention6(t, global_x) + self.a1 * p
					f1, f2, f3 = torch.split(t, [1, 1, 1], dim=-2)
					f = f1.mean(-2), f2.mean(-2), f3.mean(-2)
					f1_part = f[0] + n1.mean(-2)
					f2_part = f[1] + n2.mean(-2)
					f3_part = f[2] + n3.mean(-2)

				out_list1 = torch.zeros(4, B, self.class_num).cuda()
				out_list1[0] = self.classifier_list[0](x[0])
				out_list1[1] = self.classifier_list[1](x[1])
				out_list1[2] = self.classifier_list[2](x[2])
				out_list1[3] = self.classifier_list[3](torch.cat(x, dim=-1))

				if label is not None:
					if not no_qp:
						global loss_a, loss_b, loss_c
						out_list2 = torch.zeros(4, B, self.class_num).cuda()
						out_list2[0] = self.qp_list[0](x[0].detach())
						out_list2[1] = self.qp_list[1](x[1].detach())
						out_list2[2] = self.qp_list[2](x[2].detach())
						out_list2[3] = self.qp_list[3](torch.cat(x, dim=-1).detach())

						if self.dim == 768:
							out_list4 = torch.zeros(4, B, self.class_num).cuda()
							out_list4[0] = self.qp_list[0](f1_part.detach())
							out_list4[1] = self.qp_list[1](f2_part.detach())
							out_list4[2] = self.qp_list[2](f3_part.detach())
							out_list4[3] = self.qp_list[3](torch.cat((f1_part, f2_part, f3_part), dim=-1).detach())
						elif self.basic_dim == 512:
							out_list4 = torch.zeros(4, B * self.topn, self.class_num).cuda()
							out_list4[0] = self.qp_list[0](f1_part.detach())
							out_list4[1] = self.qp_list[1](f2_part.detach())
							out_list4[2] = self.qp_list[2](f3_part.detach())
							out_list4[3] = self.qp_list[3](torch.cat((f1_part, f2_part, f3_part), dim=-1).detach())

					if self.dim == 768:
						out_list3 = torch.zeros(4, B, self.class_num).cuda()
						out_list3[0] = self.classifier_list[0](f1_part)
						out_list3[1] = self.classifier_list[1](f2_part)
						out_list3[2] = self.classifier_list[2](f3_part)
						out_list3[3] = self.classifier_list[3](torch.cat((f1_part, f2_part, f3_part), dim=-1))
					elif self.basic_dim == 512:
						out_list3 = torch.zeros(4, B * self.topn, self.class_num).cuda()
						out_list3[0] = self.classifier_list[0](f1_part)
						out_list3[1] = self.classifier_list[1](f2_part)
						out_list3[2] = self.classifier_list[2](f3_part)
						out_list3[3] = self.classifier_list[3](torch.cat((f1_part, f2_part, f3_part), dim=-1))

					loss_img = 0
					loss_parts = 0
					if self.basic_dim == 512:
						if len(label.shape) == 2:
							num_cls = label.shape[-1]
							label_parts = label.unsqueeze(1).repeat(1, self.topn, 1)
							label_parts = label_parts.reshape(-1, num_cls)
						else:
							label_parts = label.unsqueeze(1).repeat(1, self.topn).reshape(-1)

					if no_qp:
						for criterion1, out in zip(self.criterion1, out_list1):
							loss_img += criterion1(out, label)
						for criterion1, out in zip(self.criterion1, out_list3):
							loss_parts += criterion1(out, label_parts)

					else:
						for criterion1, criterion2, out, qp_out in zip(self.criterion1, self.criterion2, out_list1, out_list2):
							loss_a = criterion1(out, label)
							loss_b = criterion2(qp_out, label)
							loss_c = self.qp_loss(out, qp_out, label, loss_a)
							loss_img += loss_a + loss_b + loss_c

						if self.basic_dim == 512:
							for criterion1, criterion2, out, qp_out in zip(self.criterion1, self.criterion2, out_list3, out_list4):
								loss_a = criterion1(out, label_parts)
								loss_b = criterion2(qp_out, label_parts)
								loss_c = self.qp_loss(out, qp_out, label_parts, loss_a)
								loss_parts += loss_a + loss_b + loss_c
						elif self.basic_dim == 768:
							for criterion1, criterion2, out, qp_out in zip(self.criterion1, self.criterion2, out_list3, out_list4):
								loss_a = criterion1(out, label)
								loss_b = criterion2(qp_out, label)
								loss_c = self.qp_loss(out, qp_out, label, loss_a)
								loss_parts += loss_a + loss_b + loss_c

					if self.basic_dim == 512:
						loss_rank = ranking_loss(top_n_prob, list_loss(out_list3[3], label_parts).reshape(B, self.topn))
						loss_parts = loss_parts + loss_rank
					elif self.dim == 768:
						loss_rank = ranking_loss(top_n_prob, list_loss(out_list3[3], label).reshape(B, self.topn))
						loss_parts = loss_parts + loss_rank

					loss = loss_img + loss_parts
					return out_list1.sum(0), [loss, loss_img, loss_parts]
				else:
					return out_list1.sum(0)

		else:
			if no_cls:
				x = self.conv(x)
				x = x[-1]
				x = self.gmp(x).reshape(B, -1)
				out_list1 = self.classifier_list[0](x)

				if label is not None:
					out_list2 = self.qp_list[0](x)
					loss_a = self.criterion2[0](out_list1, label)
					loss_b = self.criterion2[0](out_list2, label)
					loss_c = self.qp_loss(out_list1, out_list2, label, loss_a)

					loss = loss_a + loss_b + loss_c

					return out_list1, [loss]
				else:
					return out_list1
			else:
				if self.basic_dim == 512 or self.basic_dim == 256:
					x = self.conv(x)

					x[0] = self.gmp(x[0]).reshape(B, -1)
					x[1] = self.gmp(x[1]).reshape(B, -1)
					x[2] = self.gmp(x[2]).reshape(B, -1)

				out_list1 = torch.zeros(4, B, self.class_num).cuda()
				out_list1[0] = self.classifier_list[0](x[0])
				out_list1[1] = self.classifier_list[1](x[1])
				out_list1[2] = self.classifier_list[2](x[2])
				out_list1[3] = self.classifier_list[3](torch.cat(x, dim=-1))

				if label is not None:
					if not no_qp:
						out_list2 = torch.zeros(4, B, self.class_num).cuda()
						out_list2[0] = self.qp_list[0](x[0].detach())
						out_list2[1] = self.qp_list[1](x[1].detach())
						out_list2[2] = self.qp_list[2](x[2].detach())
						out_list2[3] = self.qp_list[3](torch.cat(x, dim=-1).detach())

					loss_img = 0

					if no_qp:
						for criterion1, out in zip(self.criterion1, out_list1):
							loss_img += criterion1(out, label)

					else:
						for criterion1, criterion2, out, qp_out in zip(self.criterion1, self.criterion2, out_list1, out_list2):
							loss_a = criterion1(out, label)
							loss_b = criterion2(qp_out, label)
							loss_c = self.qp_loss(out, qp_out, label, loss_a)
							loss_img += loss_a + loss_b + loss_c

					loss = loss_img
					return out_list1.sum(0), [loss, loss_img]
				else:
					return out_list1.sum(0)


class eca_layer(nn.Module):
	def __init__(self, k_size=3):
		super().__init__()
		self.avg_pool = nn.AdaptiveAvgPool2d(1)
		self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)
		self.sigmoid = nn.Sigmoid()

	def forward(self, x):
		y = self.avg_pool(x)

		# Two different branches of ECA module
		y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)

		# Multi-scale information fusion
		y = self.sigmoid(y)

		return x * y.expand_as(x)


class QPClassifier(nn.Module):
	def __init__(self, input_dim, dim, class_num):
		super().__init__()
		self.class_num = class_num
		self.cls = nn.Sequential(nn.Linear(input_dim, dim),
		                         nn.ReLU(),
		                         nn.Linear(dim, self.class_num))

	def forward(self, x):
		output = self.cls(x)
		return output


class Classifier(nn.Module):
	def __init__(self, input_dim, dim, class_num, dropout=0.5):
		super().__init__()
		self.bn1 = nn.BatchNorm1d(input_dim)
		self.bn2 = nn.BatchNorm1d(dim)
		self.dropout = nn.Dropout(dropout)
		self.relu = nn.ReLU(inplace=True)
		self.linear1 = nn.Linear(input_dim, dim)
		self.linear2 = nn.Linear(dim, class_num)

	def forward(self, x):
		B = x.shape[0]
		if B < 2:
			x = self.linear1(x)
			x = self.relu(x)
			x = self.linear2(self.dropout(x))
			return x
		if len(x.shape) == 3:
			x = self.linear1(x)
			x = self.relu(x)
			x = self.linear2(self.dropout(x))
			return x

		x = self.linear1(self.bn1(x))
		x = self.relu(self.bn2(x))
		x = self.linear2(self.dropout(x))
		return x


class SAEBlock(nn.Module):
	def __init__(self, depth):
		super().__init__()
		self.down1 = nn.Conv2d(depth, 128, 3, 1, padding=1)
		self.down2 = nn.Conv2d(128, 128, 3, 2, 1)
		self.down3 = nn.Conv2d(128, 128, 3, 2, 1)
		self.relu = nn.ReLU()
		self.tidy1 = nn.Conv2d(128, 6, 1, 1, 0)
		self.tidy2 = nn.Conv2d(128, 6, 1, 1, 0)
		self.tidy3 = nn.Conv2d(128, 9, 1, 1, 0)

		self.dconv = nn.Conv2d(128, 128, 1)
		self.gmp = nn.AdaptiveMaxPool2d(1)
		self.gap = nn.AdaptiveAvgPool2d(1)
		self.softmax = nn.Softmax(dim=1)
		self.sigmoid = nn.Sigmoid()
		self.downsample = nn.Conv2d(128, 128, 3, stride=2, padding=1)

	def forward(self, x):
		batch_size = x.size(0)
		d1 = self.relu(self.down1(x))
		d2 = self.relu(self.down2(d1))
		d3 = self.relu(self.down3(d2))

		d2_1 = self.softmax(self.dconv(self.gap(d2)))
		e2_1 = d2_1 * self.dconv(d1)
		d1_final = d1 - e2_1
		d2_2 = d2 + self.downsample(e2_1)
		d3_1 = self.softmax(self.dconv(self.gap(d3)))
		e3_1 = d3_1 * self.dconv(d2_2)
		d2_final = d2 - e3_1
		d3_final = d3 + self.downsample(e3_1)

		t1 = self.tidy1(d1_final).reshape(batch_size, -1)
		t2 = self.tidy2(d2_final).reshape(batch_size, -1)
		t3 = self.tidy3(d3_final).reshape(batch_size, -1)
		return torch.cat((t1, t2, t3), dim=1)


class PatchEmbed(nn.Module):
	def __init__(self, img_size, patch_size=7, in_c=1024, embed_dim=1024, norm_layer=None,
	             bias=True):
		super().__init__()
		img_size = (img_size, img_size)
		patch_size = (patch_size, patch_size)
		self.img_size = img_size
		self.patch_size = patch_size
		self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])
		self.num_patches = self.grid_size[0] * self.grid_size[1]

		self.proj1 = nn.Conv2d(in_c, in_c, kernel_size=patch_size, stride=patch_size, groups=in_c // 4)  # groups=in_c
		self.proj2 = nn.Conv2d(in_c, embed_dim, kernel_size=1, stride=1)
		self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

	def forward(self, x):
		B, C, H, W = x.shape
		assert H == self.img_size[0] and W == self.img_size[1], \
			f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."

		x = self.proj1(x)
		x = self.proj2(x)
		x = self.norm(x)
		return x


class ConvBlock(nn.Module):
	def __init__(self, basic_dim, dim, stage=3, same=False):
		super().__init__()
		self.conv_list = nn.ModuleList()
		self.stage = stage
		for i in range(self.stage):
			if same:
				self.conv_list.append(nn.Sequential(
					BasicConv(basic_dim * 2 ** i, basic_dim, kernel_size=1, stride=1, padding=0, relu=True),
					BasicConv(basic_dim, basic_dim * 2 ** i, kernel_size=3, stride=1, padding=1, relu=True)))
			else:
				self.conv_list.append(nn.Sequential(
					BasicConv(basic_dim * 2 ** i, basic_dim, kernel_size=1, stride=1, padding=0, relu=True),
					BasicConv(basic_dim, dim // 2, kernel_size=3, stride=1, padding=1, relu=True),))

	def forward(self, x):
		out_list, feature_weights_list = [], []

		for i in range(self.stage):
			out_list.append(
				self.conv_list[i](x[i]))
		return out_list


class Attention(nn.Module):
	def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
		super().__init__()
		assert dim % num_heads == 0, 'dim should be divisible by num_heads'
		self.num_heads = num_heads
		head_dim = dim // num_heads
		self.scale = head_dim ** -0.5

		self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
		self.attn_drop = nn.Dropout(attn_drop)
		self.proj = nn.Linear(dim, dim)
		self.proj_drop = nn.Dropout(proj_drop)

	def forward(self, x):
		B, N, C = x.shape
		qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
		q, k, v = qkv.unbind(0)

		attn = (q @ k.transpose(-2, -1)) * self.scale
		attn = attn.softmax(dim=-1)
		attn = self.attn_drop(attn)

		x = (attn @ v).transpose(1, 2).reshape(B, N, C)
		x = self.proj(x)
		x = self.proj_drop(x)
		return x


class BasicConv(nn.Module):
	def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True,
	             bn=True, bias=False):
		super(BasicConv, self).__init__()
		self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,
		                      stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)
		self.bn = nn.BatchNorm2d(out_planes, eps=1e-5,
		                         momentum=0.01, affine=True) if bn else None
		self.relu = nn.ReLU() if relu else None

	def forward(self, x):
		x = self.conv(x)
		if self.bn is not None:
			x = self.bn(x)
		if self.relu is not None:
			x = self.relu(x)
		return x


if __name__ == '__main__':
	img = 448
	n = 2
	x = torch.rand(n, 3, img, img).cuda()
	label = torch.randint(200, (n,)).cuda()
	backbone = resnet50().cuda()
	A = CurriculumConv(backbone, 512, 2048, 200, 4, 14, 0.1, 2, 0.5, 2, False).cuda()
	y, loss = A(x, label)
	print(y.shape)
